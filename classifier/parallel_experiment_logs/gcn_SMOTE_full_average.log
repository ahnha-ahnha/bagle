=== 실험 시작: gcn_SMOTE_full_average ===
GPU: 5
시작 시간: Tue Jul 15 03:05:05 UTC 2025

wandb: Currently logged in as: ahnha (ahnha_ahnha) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/user14/bagle/classifier/wandb/run-20250715_030514-czpp1bhr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run adni_ct_gcn_SMOTE_full_fold_1
wandb: ⭐️ View project at https://wandb.ai/ahnha_ahnha/adni_ct_gcn
wandb: 🚀 View run at https://wandb.ai/ahnha_ahnha/adni_ct_gcn/runs/czpp1bhr
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            epoch ▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:         test_acc ▃▆▅▄▄▆▃▃▅▆▆▄▆▂▅▆▆▅▅▆▇▇▁▇▆▆▇█▆▆▆▆▇▄▆▆█▆▆▆
wandb:       test_auroc ▁
wandb:          test_f1 ▁
wandb:        test_loss ▁▂▂▂▄▅▇▅▆▆▇▇▇▇▇▇▇█▂▃▅▅▅▅▆▅▆▆▆▆▆▆▆▆█▇▇▇▇▇
wandb: test_macro_auroc ▁
wandb:    test_macro_f1 ▁
wandb:        test_prec ▁
wandb:         test_rec ▁
wandb:        train_acc ▁▄▆▆▅▇████▅▅▆▅▆▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▅▆▇▇▆▇▆▆▆▇
wandb:       train_loss ▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:            epoch 5000
wandb:         test_acc 0.3821
wandb:       test_auroc 0.5779
wandb:          test_f1 0.2699
wandb:        test_loss 2.93105
wandb: test_macro_auroc 0.5779
wandb:    test_macro_f1 0.2699
wandb:        test_prec 0.2774
wandb:         test_rec 0.2963
wandb:        train_acc 0.8564
wandb:       train_loss 0.37432
wandb: 
wandb: 🚀 View run adni_ct_gcn_SMOTE_full_fold_1 at: https://wandb.ai/ahnha_ahnha/adni_ct_gcn/runs/czpp1bhr
wandb: ⭐️ View project at: https://wandb.ai/ahnha_ahnha/adni_ct_gcn
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250715_030514-czpp1bhr/logs
Auto-generated run_name: adni_ct_gcn_SMOTE_full
Loading data from: /home/user14/bagle/data/ADNI_CT/real.pt
Data loaded successfully:
  Samples: 1644
  Features shape: torch.Size([1644, 160, 1])
  Labels shape: torch.Size([1644])
  Unique labels: tensor([0, 1, 2, 3, 4])
  Fold values: tensor([0, 1, 2, 3, 4])
Number of classes: 5
Using existing fold information for K-fold cross validation
Fold 0: 1309 train samples, 335 test samples
Fold 1: 1316 train samples, 328 test samples
Fold 2: 1331 train samples, 313 test samples
Fold 3: 1308 train samples, 336 test samples
Fold 4: 1312 train samples, 332 test samples
save directory:  ./logs/20250715_030510


=============================== Fold 1 ===============================
Loaded synthetic data from SMOTE_full_fold0.pt: 1386 samples
SMOTE synthetic data - using class-based adjacency matrices
Using SMOTE_full augmentation: 1386 additional samples
Applying GCN-specific adjacency processing to real data
Applying percentile-based sparsification and normalization for GCN (normalize=False)
Adding 1386 augmented samples to training set
Generating adjacency matrices for 1386 synthetic samples using average method
Creating class-wise average adjacency matrices with 90.0% sparsification
Class 0: processed 385 matrices, original avg degree: 1.05, target edges: 84
Class 1: processed 153 matrices, original avg degree: 1.05, target edges: 84
Class 2: processed 384 matrices, original avg degree: 1.05, target edges: 84
Class 3: processed 249 matrices, original avg degree: 1.05, target edges: 84
Class 4: processed 138 matrices, original avg degree: 1.05, target edges: 84
Assigning class average adjacency matrices for 1386 synthetic samples (Option-M)
Average adjacency assignment:
  Class 0: 154 synthetic samples
  Class 1: 386 synthetic samples
  Class 2: 155 synthetic samples
  Class 3: 290 synthetic samples
  Class 4: 401 synthetic samples
Final training set size: 2695 samples

GNN Epoch [100 / 5000] loss_train: 0.95667 accuracy_train: 0.63822
 test_loss: 1.6407 test_acc: 0.2866

GNN Epoch [200 / 5000] loss_train: 0.66301 accuracy_train: 0.76289
 test_loss: 1.8296 test_acc: 0.2985

GNN Epoch [300 / 5000] loss_train: 1.04345 accuracy_train: 0.62301
 test_loss: 2.0319 test_acc: 0.2358

GNN Epoch [400 / 5000] loss_train: 0.96398 accuracy_train: 0.64045
 test_loss: 1.6948 test_acc: 0.2448

GNN Epoch [500 / 5000] loss_train: 0.84752 accuracy_train: 0.65343
 test_loss: 2.1316 test_acc: 0.2716

GNN Epoch [600 / 5000] loss_train: 0.41780 accuracy_train: 0.84824
 test_loss: 2.3603 test_acc: 0.2836

GNN Epoch [700 / 5000] loss_train: 0.46197 accuracy_train: 0.84712
 test_loss: 2.1200 test_acc: 0.2866

GNN Epoch [800 / 5000] loss_train: 0.33957 accuracy_train: 0.88052
 test_loss: 2.5386 test_acc: 0.2687

GNN Epoch [900 / 5000] loss_train: 1.10113 accuracy_train: 0.59852
 test_loss: 2.0535 test_acc: 0.3194

GNN Epoch [1000 / 5000] loss_train: 0.35369 accuracy_train: 0.87087
 test_loss: 2.4784 test_acc: 0.3015

GNN Epoch [1100 / 5000] loss_train: 0.60607 accuracy_train: 0.75510
 test_loss: 2.7324 test_acc: 0.3254

GNN Epoch [1200 / 5000] loss_train: 0.26615 accuracy_train: 0.90835
 test_loss: 2.8095 test_acc: 0.3134

GNN Epoch [1300 / 5000] loss_train: 0.25052 accuracy_train: 0.91763
 test_loss: 3.1252 test_acc: 0.2687

GNN Epoch [1400 / 5000] loss_train: 0.35494 accuracy_train: 0.87495
 test_loss: 2.6994 test_acc: 0.2866

GNN Epoch [1500 / 5000] loss_train: 0.50026 accuracy_train: 0.81596
 test_loss: 3.0944 test_acc: 0.2567

GNN Epoch [1600 / 5000] loss_train: 0.26323 accuracy_train: 0.90946
 test_loss: 2.8018 test_acc: 0.3134

GNN Epoch [1700 / 5000] loss_train: 0.22325 accuracy_train: 0.92022
 test_loss: 3.0208 test_acc: 0.3104

GNN Epoch [1800 / 5000] loss_train: 0.34599 accuracy_train: 0.87347
 test_loss: 2.7135 test_acc: 0.3134

GNN Epoch [1900 / 5000] loss_train: 0.26025 accuracy_train: 0.90983
 test_loss: 2.9669 test_acc: 0.3433

GNN Epoch [2000 / 5000] loss_train: 0.24898 accuracy_train: 0.90872
 test_loss: 3.1266 test_acc: 0.3045

GNN Epoch [2100 / 5000] loss_train: 0.91397 accuracy_train: 0.65009
 test_loss: 1.6399 test_acc: 0.3134

GNN Epoch [2200 / 5000] loss_train: 0.61454 accuracy_train: 0.79035
 test_loss: 2.1067 test_acc: 0.2836

GNN Epoch [2300 / 5000] loss_train: 0.54690 accuracy_train: 0.79703
 test_loss: 2.3644 test_acc: 0.3045

GNN Epoch [2400 / 5000] loss_train: 0.54545 accuracy_train: 0.80334
 test_loss: 2.4795 test_acc: 0.3015

GNN Epoch [2500 / 5000] loss_train: 0.51540 accuracy_train: 0.81670
 test_loss: 2.4900 test_acc: 0.3254

GNN Epoch [2600 / 5000] loss_train: 0.50364 accuracy_train: 0.80928
 test_loss: 2.6637 test_acc: 0.3403

GNN Epoch [2700 / 5000] loss_train: 0.50338 accuracy_train: 0.82152
 test_loss: 2.5072 test_acc: 0.3493

GNN Epoch [2800 / 5000] loss_train: 0.43995 accuracy_train: 0.84304
 test_loss: 2.5006 test_acc: 0.3224

GNN Epoch [2900 / 5000] loss_train: 0.49906 accuracy_train: 0.80334
 test_loss: 2.5644 test_acc: 0.3313

GNN Epoch [3000 / 5000] loss_train: 0.59172 accuracy_train: 0.76920
 test_loss: 3.0886 test_acc: 0.2299

GNN Epoch [3100 / 5000] loss_train: 0.44386 accuracy_train: 0.83748
 test_loss: 2.6158 test_acc: 0.3224

GNN Epoch [3200 / 5000] loss_train: 0.40272 accuracy_train: 0.85158
 test_loss: 2.6857 test_acc: 0.3373

GNN Epoch [3300 / 5000] loss_train: 1.60014 accuracy_train: 0.50946
 test_loss: 2.6542 test_acc: 0.2716

GNN Epoch [3400 / 5000] loss_train: 0.47403 accuracy_train: 0.81521
 test_loss: 2.5852 test_acc: 0.3224

GNN Epoch [3500 / 5000] loss_train: 0.58262 accuracy_train: 0.76252
 test_loss: 2.9587 test_acc: 0.2328

GNN Epoch [3600 / 5000] loss_train: 0.38985 accuracy_train: 0.85714
 test_loss: 2.7457 test_acc: 0.3075

GNN Epoch [3700 / 5000] loss_train: 0.36933 accuracy_train: 0.86790
 test_loss: 2.7473 test_acc: 0.3254

GNN Epoch [3800 / 5000] loss_train: 0.38589 accuracy_train: 0.85343
 test_loss: 2.7861 test_acc: 0.3164

GNN Epoch [3900 / 5000] loss_train: 0.39458 accuracy_train: 0.84824
 test_loss: 2.9553 test_acc: 0.3821
 [Saved best model with acc 0.3821 to ./logs/20250715_030510/0.pth]

GNN Epoch [4000 / 5000] loss_train: 0.44621 accuracy_train: 0.83599
 test_loss: 2.4077 test_acc: 0.3313

GNN Epoch [4100 / 5000] loss_train: 0.45877 accuracy_train: 0.82672
 test_loss: 2.7288 test_acc: 0.3164

GNN Epoch [4200 / 5000] loss_train: 0.88124 accuracy_train: 0.64341
 test_loss: 2.7879 test_acc: 0.2418

GNN Epoch [4300 / 5000] loss_train: 0.39739 accuracy_train: 0.85677
 test_loss: 2.8280 test_acc: 0.2955

GNN Epoch [4400 / 5000] loss_train: 0.40992 accuracy_train: 0.85380
 test_loss: 2.9481 test_acc: 0.3075

GNN Epoch [4500 / 5000] loss_train: 0.38059 accuracy_train: 0.85121
 test_loss: 2.9071 test_acc: 0.3194

GNN Epoch [4600 / 5000] loss_train: 0.37662 accuracy_train: 0.85269
 test_loss: 2.8392 test_acc: 0.3015

GNN Epoch [4700 / 5000] loss_train: 0.38062 accuracy_train: 0.86271
 test_loss: 2.9067 test_acc: 0.3015

GNN Epoch [4800 / 5000] loss_train: 0.38687 accuracy_train: 0.85603
 test_loss: 2.6688 test_acc: 0.3045

GNN Epoch [4900 / 5000] loss_train: 0.35664 accuracy_train: 0.86902
 test_loss: 2.8719 test_acc: 0.3075

GNN Epoch [5000 / 5000] loss_train: 0.37432 accuracy_train: 0.85640
 test_loss: 2.9310 test_acc: 0.3075
GNN Training completed! Best accuracy: 0.3821
GNN load_and_test called with model_path: ./logs/20250715_030510/0.pth
Loading GNN model from: ./logs/20250715_030510/0.pth
GPU memory cleared after fold 1


=============================== Fold 2 ===============================
Loaded synthetic data from SMOTE_full_fold1.pt: 1379 samples
SMOTE synthetic data - using class-based adjacency matrices
Using SMOTE_full augmentation: 1379 additional samples
Applying GCN-specific adjacency processing to real data
Applying percentile-based sparsification and normalization for GCN (normalize=False)
Adding 1379 augmented samples to training set
Generating adjacency matrices for 1379 synthetic samples using average method
Creating class-wise average adjacency matrices with 90.0% sparsification
Class 0: processed 380 matrices, original avg degree: 1.05, target edges: 84
Class 1: processed 152 matrices, original avg degree: 1.05, target edges: 84wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/user14/bagle/classifier/wandb/run-20250715_030821-igxl865n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run adni_ct_gcn_SMOTE_full_fold_2
wandb: ⭐️ View project at https://wandb.ai/ahnha_ahnha/adni_ct_gcn
wandb: 🚀 View run at https://wandb.ai/ahnha_ahnha/adni_ct_gcn/runs/igxl865n
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            epoch ▁▁▁▁▁▁▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇██
wandb:         test_acc █▇█▃█▅▆▆▆██▅▇▄▅▆▅▅▇▇▇█▅▃▆▆▅▇▆▅█▇▅▃▄▁▂▅▄▂
wandb:       test_auroc ▁
wandb:          test_f1 ▁
wandb:        test_loss ▁▁▃▂▂▃█▃▄▅▆▆▆█▇▇▇█▃▄▅▅▆▆▆▇▇▄▅▅▆▅▆▇▇▅▅▅▆▅
wandb: test_macro_auroc ▁
wandb:    test_macro_f1 ▁
wandb:        test_prec ▁
wandb:         test_rec ▁
wandb:        train_acc ▁▃▅▅▅▇▆▇▆▇▇██▇██▆██▇▆▆▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:       train_loss █▇▆▃▄▄▂▃▂▂▂▂▂▁▁▃▂▁▁▅▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▆▃▂▃▂
wandb: 
wandb: Run summary:
wandb:            epoch 5000
wandb:         test_acc 0.375
wandb:       test_auroc 0.596
wandb:          test_f1 0.4136
wandb:        test_loss 2.7964
wandb: test_macro_auroc 0.596
wandb:    test_macro_f1 0.2482
wandb:        test_prec 0.289
wandb:         test_rec 0.2483
wandb:        train_acc 0.86605
wandb:       train_loss 0.39115
wandb: 
wandb: 🚀 View run adni_ct_gcn_SMOTE_full_fold_2 at: https://wandb.ai/ahnha_ahnha/adni_ct_gcn/runs/igxl865n
wandb: ⭐️ View project at: https://wandb.ai/ahnha_ahnha/adni_ct_gcn
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250715_030821-igxl865n/logs
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/user14/bagle/classifier/wandb/run-20250715_031122-9fv735sm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run adni_ct_gcn_SMOTE_full_fold_3
wandb: ⭐️ View project at https://wandb.ai/ahnha_ahnha/adni_ct_gcn
wandb: 🚀 View run at https://wandb.ai/ahnha_ahnha/adni_ct_gcn/runs/9fv735sm

Class 2: processed 404 matrices, original avg degree: 1.05, target edges: 84
Class 3: processed 248 matrices, original avg degree: 1.05, target edges: 84
Class 4: processed 132 matrices, original avg degree: 1.05, target edges: 84
Assigning class average adjacency matrices for 1379 synthetic samples (Option-M)
Average adjacency assignment:
  Class 0: 159 synthetic samples
  Class 1: 387 synthetic samples
  Class 2: 135 synthetic samples
  Class 3: 291 synthetic samples
  Class 4: 407 synthetic samples
Final training set size: 2695 samples

GNN Epoch [100 / 5000] loss_train: 0.90757 accuracy_train: 0.66568
 test_loss: 1.6610 test_acc: 0.2805

GNN Epoch [200 / 5000] loss_train: 0.87740 accuracy_train: 0.68497
 test_loss: 1.6109 test_acc: 0.3293

GNN Epoch [300 / 5000] loss_train: 0.76848 accuracy_train: 0.70761
 test_loss: 1.7838 test_acc: 0.3079

GNN Epoch [400 / 5000] loss_train: 0.60457 accuracy_train: 0.78701
 test_loss: 1.9777 test_acc: 0.2683

GNN Epoch [500 / 5000] loss_train: 0.58120 accuracy_train: 0.78627
 test_loss: 2.0370 test_acc: 0.2622

GNN Epoch [600 / 5000] loss_train: 0.50172 accuracy_train: 0.81447
 test_loss: 2.0689 test_acc: 0.2622

GNN Epoch [700 / 5000] loss_train: 0.52075 accuracy_train: 0.78924
 test_loss: 2.3883 test_acc: 0.2470

GNN Epoch [800 / 5000] loss_train: 0.45979 accuracy_train: 0.82375
 test_loss: 2.6329 test_acc: 0.2622

GNN Epoch [900 / 5000] loss_train: 0.55116 accuracy_train: 0.79555
 test_loss: 2.5692 test_acc: 0.2927

GNN Epoch [1000 / 5000] loss_train: 0.68579 accuracy_train: 0.74583
 test_loss: 2.2148 test_acc: 0.2805

GNN Epoch [1100 / 5000] loss_train: 0.30543 accuracy_train: 0.89091
 test_loss: 2.8035 test_acc: 0.2927

GNN Epoch [1200 / 5000] loss_train: 0.48420 accuracy_train: 0.81744
 test_loss: 2.4484 test_acc: 0.2866

GNN Epoch [1300 / 5000] loss_train: 0.33696 accuracy_train: 0.87347
 test_loss: 2.7988 test_acc: 0.2988

GNN Epoch [1400 / 5000] loss_train: 0.87216 accuracy_train: 0.67124
 test_loss: 2.2320 test_acc: 0.2652

GNN Epoch [1500 / 5000] loss_train: 0.39987 accuracy_train: 0.85751
 test_loss: 2.6502 test_acc: 0.2713

GNN Epoch [1600 / 5000] loss_train: 0.31999 accuracy_train: 0.88720
 test_loss: 2.8072 test_acc: 0.3140

GNN Epoch [1700 / 5000] loss_train: 0.36678 accuracy_train: 0.86382
 test_loss: 3.0748 test_acc: 0.3232

GNN Epoch [1800 / 5000] loss_train: 0.30688 accuracy_train: 0.89017
 test_loss: 2.8357 test_acc: 0.2927

GNN Epoch [1900 / 5000] loss_train: 0.25083 accuracy_train: 0.91540
 test_loss: 2.9886 test_acc: 0.2805

GNN Epoch [2000 / 5000] loss_train: 0.50234 accuracy_train: 0.81633
 test_loss: 4.1549 test_acc: 0.2073

GNN Epoch [2100 / 5000] loss_train: 0.36954 accuracy_train: 0.86827
 test_loss: 2.3888 test_acc: 0.2744

GNN Epoch [2200 / 5000] loss_train: 0.29533 accuracy_train: 0.89314
 test_loss: 2.7733 test_acc: 0.2957

GNN Epoch [2300 / 5000] loss_train: 0.29198 accuracy_train: 0.89796
 test_loss: 2.7954 test_acc: 0.3140

GNN Epoch [2400 / 5000] loss_train: 0.25441 accuracy_train: 0.90353
 test_loss: 2.9868 test_acc: 0.2866

GNN Epoch [2500 / 5000] loss_train: 1.13983 accuracy_train: 0.52282
 test_loss: 1.9099 test_acc: 0.2287

GNN Epoch [2600 / 5000] loss_train: 0.53445 accuracy_train: 0.80111
 test_loss: 2.3686 test_acc: 0.2713

GNN Epoch [2700 / 5000] loss_train: 0.50550 accuracy_train: 0.80853
 test_loss: 2.5559 test_acc: 0.2805

GNN Epoch [2800 / 5000] loss_train: 0.43473 accuracy_train: 0.84267
 test_loss: 2.7176 test_acc: 0.2683

GNN Epoch [2900 / 5000] loss_train: 0.42392 accuracy_train: 0.84564
 test_loss: 2.8200 test_acc: 0.2988

GNN Epoch [3000 / 5000] loss_train: 0.49076 accuracy_train: 0.81150
 test_loss: 3.1380 test_acc: 0.2805

GNN Epoch [3100 / 5000] loss_train: 0.44036 accuracy_train: 0.83377
 test_loss: 2.7958 test_acc: 0.2774

GNN Epoch [3200 / 5000] loss_train: 0.38082 accuracy_train: 0.86308
 test_loss: 2.8697 test_acc: 0.2896

GNN Epoch [3300 / 5000] loss_train: 0.48161 accuracy_train: 0.82672
 test_loss: 2.3549 test_acc: 0.2744

GNN Epoch [3400 / 5000] loss_train: 0.52847 accuracy_train: 0.79963
 test_loss: 2.2360 test_acc: 0.2927

GNN Epoch [3500 / 5000] loss_train: 0.47847 accuracy_train: 0.83414
 test_loss: 2.5314 test_acc: 0.2622

GNN Epoch [3600 / 5000] loss_train: 0.50627 accuracy_train: 0.81447
 test_loss: 2.3439 test_acc: 0.2561

GNN Epoch [3700 / 5000] loss_train: 0.39231 accuracy_train: 0.85566
 test_loss: 2.7008 test_acc: 0.2774

GNN Epoch [3800 / 5000] loss_train: 0.38732 accuracy_train: 0.85455
 test_loss: 2.7117 test_acc: 0.2530

GNN Epoch [3900 / 5000] loss_train: 0.37443 accuracy_train: 0.86085
 test_loss: 2.8531 test_acc: 0.2561

GNN Epoch [4000 / 5000] loss_train: 0.48809 accuracy_train: 0.81521
 test_loss: 2.7320 test_acc: 0.2043

GNN Epoch [4100 / 5000] loss_train: 0.38304 accuracy_train: 0.86085
 test_loss: 2.9459 test_acc: 0.2439

GNN Epoch [4200 / 5000] loss_train: 0.41773 accuracy_train: 0.84861
 test_loss: 2.7183 test_acc: 0.2561

GNN Epoch [4300 / 5000] loss_train: 0.41240 accuracy_train: 0.85158
 test_loss: 2.6417 test_acc: 0.2409

GNN Epoch [4400 / 5000] loss_train: 0.41872 accuracy_train: 0.84675
 test_loss: 2.5800 test_acc: 0.2439

GNN Epoch [4500 / 5000] loss_train: 0.38564 accuracy_train: 0.86271
 test_loss: 2.7536 test_acc: 0.2500

GNN Epoch [4600 / 5000] loss_train: 0.37419 accuracy_train: 0.86308
 test_loss: 2.9211 test_acc: 0.2378

GNN Epoch [4700 / 5000] loss_train: 0.51169 accuracy_train: 0.80891
 test_loss: 2.6025 test_acc: 0.2470

GNN Epoch [4800 / 5000] loss_train: 0.41340 accuracy_train: 0.84750
 test_loss: 2.7938 test_acc: 0.2713

GNN Epoch [4900 / 5000] loss_train: 0.39100 accuracy_train: 0.85826
 test_loss: 2.6804 test_acc: 0.2591

GNN Epoch [5000 / 5000] loss_train: 0.39115 accuracy_train: 0.86605
 test_loss: 2.7964 test_acc: 0.2104
GNN Training completed! Best accuracy: 0.3750
GNN load_and_test called with model_path: ./logs/20250715_030510/1.pth
Loading GNN model from: ./logs/20250715_030510/1.pth
GPU memory cleared after fold 2


=============================== Fold 3 ===============================
Loaded synthetic data from SMOTE_full_fold2.pt: 1364 samples
SMOTE synthetic data - using class-based adjacency matrices
Using SMOTE_full augmentation: 1364 additional samples
Applying GCN-specific adjacency processing to real data
Applying percentile-based sparsification and normalization for GCN (normalize=False)
Adding 1364 augmented samples to training set
Generating adjacency matrices for 1364 synthetic samples using average method
Creating class-wise average adjacency matrices with 90.0% sparsification
Class 0: processed 361 matrices, original avg degree: 1.05, target edges: 84
Class 1: processed 155 matrices, original avg degree: 1.05, target edges: 84
Class 2: processed 424 matrices, original avg degree: 1.05, target edges: 84
Class 3: processed 260 matrices, original avg degree: 1.05, target edges: 84
Class 4: processed 131 matrices, original avg degree: 1.05, target edges: 84
Assigning class average adjacency matrices for 1364 synthetic samples (Option-M)
Average adjacency assignment:
  Class 0: 178 synthetic samples
  Class 1: 384 synthetic samples
  Class 2: 115 synthetic samples
  Class 3: 279 synthetic samples
  Class 4: 408 synthetic samples
Final training set size: 2695 samples

GNN Epoch [100 / 5000] loss_train: 1.06587 accuracy_train: 0.57848
 test_loss: 1.6897 test_acc: 0.3035

GNN Epoch [200 / 5000] loss_train: 0.84754 accuracy_train: 0.65417
 test_loss: 1.8323 test_acc: 0.2173

GNN Epoch [300 / 5000] loss_train: 0.41446 accuracy_train: 0.85863
 test_loss: 2.1514 test_acc: 0.3163

GNN Epoch [400 / 5000] loss_train: 0.22497 accuracy_train: 0.93061
 test_loss: 2.3670 test_acc: 0.2939

GNN Epoch [500 / 5000] loss_train: 0.25814 accuracy_train: 0.89944
 test_loss: 2.7435 test_acc: 0.2907

GNN Epoch [600 / 5000] loss_train: 0.20615 accuracy_train: 0.93432
 test_loss: 2.3889 test_acc: 0.2748

GNN Epoch [700 / 5000] loss_train: 0.84951 accuracy_train: 0.67273
 test_loss: 1.6159 test_acc: 0.3035

GNN Epoch [800 / 5000] loss_train: 0.50533 accuracy_train: 0.81521
 test_loss: 2.1824 test_acc: 0.2748

GNN Epoch [900 / 5000] loss_train: 0.33132 accuracy_train: 0.87904
wandb: uploading config.yaml
wandb: uploading history steps 4694-5000, summary, console lines 138-152
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████
wandb:         test_acc ▆▄▆▅▆▃▁▄▇▃▄▇▄▇▅▅▆▂▅▅▄▄▅▃▅▆▆▅▇█▆▄▅▅▄▅▆▄▇▅
wandb:       test_auroc ▁
wandb:          test_f1 ▁
wandb:        test_loss ▁▁▂▆▅▅▆▆▁▃▃▄▅▆▄▇▆▇▇▇▆▇██▆▆▃▅▆▇▆▇▇▆▆█▆▆▆▇
wandb: test_macro_auroc ▁
wandb:    test_macro_f1 ▁
wandb:        test_prec ▁
wandb:         test_rec ▁
wandb:        train_acc ▁▄▆▇█▆█▆▆▆▅▇▇▇▆▇██▇▇▇▇█▇██▆▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:       train_loss █▅▇▃▁▂▃▂▁▁▁▁▁▁▁▁▃▁▁▂▁▁▁▁▂▂▁▂▁▂▂▂▂▂▁▂▁▁▁▃
wandb: 
wandb: Run summary:
wandb:            epoch 5000
wandb:         test_acc 0.3706
wandb:       test_auroc 0.6203
wandb:          test_f1 0.3351
wandb:        test_loss 2.70931
wandb: test_macro_auroc 0.6203
wandb:    test_macro_f1 0.2681
wandb:        test_prec 0.2801
wandb:         test_rec 0.3222
wandb:        train_acc 0.87644
wandb:       train_loss 0.34318
wandb: 
wandb: 🚀 View run adni_ct_gcn_SMOTE_full_fold_3 at: https://wandb.ai/ahnha_ahnha/adni_ct_gcn/runs/9fv735sm
wandb: ⭐️ View project at: https://wandb.ai/ahnha_ahnha/adni_ct_gcn
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250715_031122-9fv735sm/logs
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/user14/bagle/classifier/wandb/run-20250715_031422-ql9q11gu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run adni_ct_gcn_SMOTE_full_fold_4
wandb: ⭐️ View project at https://wandb.ai/ahnha_ahnha/adni_ct_gcn
wandb: 🚀 View run at https://wandb.ai/ahnha_ahnha/adni_ct_gcn/runs/ql9q11gu
 test_loss: 2.3823 test_acc: 0.2907

GNN Epoch [1000 / 5000] loss_train: 0.67392 accuracy_train: 0.73395
 test_loss: 2.0112 test_acc: 0.2939

GNN Epoch [1100 / 5000] loss_train: 0.38965 accuracy_train: 0.86011
 test_loss: 2.2765 test_acc: 0.2396

GNN Epoch [1200 / 5000] loss_train: 0.38861 accuracy_train: 0.86160
 test_loss: 2.3717 test_acc: 0.3099

GNN Epoch [1300 / 5000] loss_train: 0.38087 accuracy_train: 0.85083
 test_loss: 3.3456 test_acc: 0.2556

GNN Epoch [1400 / 5000] loss_train: 0.35003 accuracy_train: 0.87199
 test_loss: 2.8526 test_acc: 0.2588

GNN Epoch [1500 / 5000] loss_train: 0.39451 accuracy_train: 0.85900
 test_loss: 2.2701 test_acc: 0.2716

GNN Epoch [1600 / 5000] loss_train: 0.32676 accuracy_train: 0.88757
 test_loss: 2.5444 test_acc: 0.2875

GNN Epoch [1700 / 5000] loss_train: 0.30903 accuracy_train: 0.88942
 test_loss: 2.7492 test_acc: 0.3035

GNN Epoch [1800 / 5000] loss_train: 0.28903 accuracy_train: 0.89796
 test_loss: 2.6116 test_acc: 0.3163

GNN Epoch [1900 / 5000] loss_train: 0.33502 accuracy_train: 0.88052
 test_loss: 2.4751 test_acc: 0.2748

GNN Epoch [2000 / 5000] loss_train: 0.26200 accuracy_train: 0.91132
 test_loss: 2.7613 test_acc: 0.2971

GNN Epoch [2100 / 5000] loss_train: 0.39576 accuracy_train: 0.84453
 test_loss: 2.7807 test_acc: 0.2971

GNN Epoch [2200 / 5000] loss_train: 0.45900 accuracy_train: 0.81224
 test_loss: 3.0763 test_acc: 0.2588

GNN Epoch [2300 / 5000] loss_train: 0.39023 accuracy_train: 0.86382
 test_loss: 2.6006 test_acc: 0.2971

GNN Epoch [2400 / 5000] loss_train: 0.34755 accuracy_train: 0.86308
 test_loss: 2.6449 test_acc: 0.2620

GNN Epoch [2500 / 5000] loss_train: 0.24473 accuracy_train: 0.91354
 test_loss: 2.8573 test_acc: 0.2843

GNN Epoch [2600 / 5000] loss_train: 0.24931 accuracy_train: 0.91911
 test_loss: 2.6839 test_acc: 0.2780

GNN Epoch [2700 / 5000] loss_train: 0.24571 accuracy_train: 0.91058
 test_loss: 2.8693 test_acc: 0.2620

GNN Epoch [2800 / 5000] loss_train: 0.23546 accuracy_train: 0.91243
 test_loss: 2.9217 test_acc: 0.2716

GNN Epoch [2900 / 5000] loss_train: 0.22775 accuracy_train: 0.92801
 test_loss: 2.7039 test_acc: 0.2748

GNN Epoch [3000 / 5000] loss_train: 0.27407 accuracy_train: 0.89907
 test_loss: 2.7971 test_acc: 0.2780

GNN Epoch [3100 / 5000] loss_train: 0.21947 accuracy_train: 0.92839
 test_loss: 2.9257 test_acc: 0.2620

GNN Epoch [3200 / 5000] loss_train: 0.42577 accuracy_train: 0.84267
 test_loss: 2.3785 test_acc: 0.3099

GNN Epoch [3300 / 5000] loss_train: 0.34714 accuracy_train: 0.87236
 test_loss: 2.6111 test_acc: 0.2556

GNN Epoch [3400 / 5000] loss_train: 0.40107 accuracy_train: 0.85121
 test_loss: 2.5129 test_acc: 0.2812

GNN Epoch [3500 / 5000] loss_train: 0.32096 accuracy_train: 0.88980
 test_loss: 2.6871 test_acc: 0.2652

GNN Epoch [3600 / 5000] loss_train: 0.38124 accuracy_train: 0.86085
 test_loss: 2.6461 test_acc: 0.3067

GNN Epoch [3700 / 5000] loss_train: 0.37867 accuracy_train: 0.86234
 test_loss: 2.7496 test_acc: 0.2492

GNN Epoch [3800 / 5000] loss_train: 0.35261 accuracy_train: 0.87050
 test_loss: 2.6108 test_acc: 0.2843

GNN Epoch [3900 / 5000] loss_train: 0.36339 accuracy_train: 0.87161
 test_loss: 2.5846 test_acc: 0.2971

GNN Epoch [4000 / 5000] loss_train: 0.34990 accuracy_train: 0.86494
 test_loss: 2.8370 test_acc: 0.2492

GNN Epoch [4100 / 5000] loss_train: 0.57973 accuracy_train: 0.78145
 test_loss: 2.6232 test_acc: 0.2716

GNN Epoch [4200 / 5000] loss_train: 0.32950 accuracy_train: 0.87978
 test_loss: 2.7597 test_acc: 0.2748

GNN Epoch [4300 / 5000] loss_train: 0.82007 accuracy_train: 0.70501
 test_loss: 3.2036 test_acc: 0.2971

GNN Epoch [4400 / 5000] loss_train: 0.32560 accuracy_train: 0.88237
 test_loss: 2.5590 test_acc: 0.3067

GNN Epoch [4500 / 5000] loss_train: 0.35220 accuracy_train: 0.87199
 test_loss: 2.8879 test_acc: 0.2556

GNN Epoch [4600 / 5000] loss_train: 0.31023 accuracy_train: 0.88757
 test_loss: 2.7610 test_acc: 0.2652

GNN Epoch [4700 / 5000] loss_train: 0.30349 accuracy_train: 0.89128
 test_loss: 2.5941 test_acc: 0.2556

GNN Epoch [4800 / 5000] loss_train: 0.30462 accuracy_train: 0.89202
 test_loss: 2.5765 test_acc: 0.2588

GNN Epoch [4900 / 5000] loss_train: 0.29076 accuracy_train: 0.90167
 test_loss: 2.7489 test_acc: 0.2652

GNN Epoch [5000 / 5000] loss_train: 0.34318 accuracy_train: 0.87644
 test_loss: 2.7093 test_acc: 0.2620
GNN Training completed! Best accuracy: 0.3706
GNN load_and_test called with model_path: ./logs/20250715_030510/2.pth
Loading GNN model from: ./logs/20250715_030510/2.pth
GPU memory cleared after fold 3


=============================== Fold 4 ===============================
Loaded synthetic data from SMOTE_full_fold3.pt: 1387 samples
SMOTE synthetic data - using class-based adjacency matrices
Using SMOTE_full augmentation: 1387 additional samples
Applying GCN-specific adjacency processing to real data
Applying percentile-based sparsification and normalization for GCN (normalize=False)
Adding 1387 augmented samples to training set
Generating adjacency matrices for 1387 synthetic samples using average method
Creating class-wise average adjacency matrices with 90.0% sparsification
Class 0: processed 371 matrices, original avg degree: 1.05, target edges: 84
Class 1: processed 153 matrices, original avg degree: 1.05, target edges: 84
Class 2: processed 407 matrices, original avg degree: 1.05, target edges: 84
Class 3: processed 241 matrices, original avg degree: 1.05, target edges: 84
Class 4: processed 136 matrices, original avg degree: 1.05, target edges: 84
Assigning class average adjacency matrices for 1387 synthetic samples (Option-M)
Average adjacency assignment:
  Class 0: 168 synthetic samples
  Class 1: 386 synthetic samples
  Class 2: 132 synthetic samples
  Class 3: 298 synthetic samples
  Class 4: 403 synthetic samples
Final training set size: 2695 samples

GNN Epoch [100 / 5000] loss_train: 1.00686 accuracy_train: 0.60334
 test_loss: 1.7328 test_acc: 0.2262

GNN Epoch [200 / 5000] loss_train: 0.85323 accuracy_train: 0.67681
 test_loss: 1.6531 test_acc: 0.3333

GNN Epoch [300 / 5000] loss_train: 0.71532 accuracy_train: 0.73061
 test_loss: 1.8187 test_acc: 0.3304

GNN Epoch [400 / 5000] loss_train: 0.54899 accuracy_train: 0.80853
 test_loss: 2.0418 test_acc: 0.3006

GNN Epoch [500 / 5000] loss_train: 0.43658 accuracy_train: 0.85380
 test_loss: 1.9156 test_acc: 0.3006

GNN Epoch [600 / 5000] loss_train: 0.37093 accuracy_train: 0.88312
 test_loss: 2.1114 test_acc: 0.2768

GNN Epoch [700 / 5000] loss_train: 0.38140 accuracy_train: 0.87458
 test_loss: 2.1005 test_acc: 0.2708

GNN Epoch [800 / 5000] loss_train: 0.29951 accuracy_train: 0.89536
 test_loss: 2.6201 test_acc: 0.2619

GNN Epoch [900 / 5000] loss_train: 0.63175 accuracy_train: 0.76475
 test_loss: 2.2533 test_acc: 0.2202

GNN Epoch [1000 / 5000] loss_train: 0.40872 accuracy_train: 0.85158
 test_loss: 2.2353 test_acc: 0.2887

GNN Epoch [1100 / 5000] loss_train: 0.68599 accuracy_train: 0.72245
 test_loss: 2.0031 test_acc: 0.2768

GNN Epoch [1200 / 5000] loss_train: 0.51751 accuracy_train: 0.80742
 test_loss: 2.2550 test_acc: 0.3006

GNN Epoch [1300 / 5000] loss_train: 0.55109 accuracy_train: 0.78516
 test_loss: 2.4165 test_acc: 0.3155

GNN Epoch [1400 / 5000] loss_train: 0.58728 accuracy_train: 0.75696
 test_loss: 2.3471 test_acc: 0.2619

GNN Epoch [1500 / 5000] loss_train: 0.53452 accuracy_train: 0.78516
 test_loss: 2.2730 test_acc: 0.2738

GNN Epoch [1600 / 5000] loss_train: 0.53488 accuracy_train: 0.79703
 test_loss: 2.4700 test_acc: 0.3006

GNN Epoch [1700 / 5000] loss_train: 0.45465 accuracy_train: 0.83228
 test_loss: 2.5906 test_acc: 0.2798

GNN Epoch [1800 / 5000] loss_train: 0.47483 accuracy_train: 0.81596
 test_loss: 2.5003 test_acc: 0.2708

GNN Epoch [1900 / 5000] loss_train: 0.42294 accuracy_train: 0.84007
 test_loss: 2.6676 test_acc: 0.2738

GNN Epoch [2000 / 5000] loss_train: 0.38690 accuracy_train: 0.85566
 test_loss: 2.6185 test_acc: 0.2768

GNN Epoch [2100 / 5000] loss_train: 0.38274 accuracy_train: 0.85788
 test_loss: 2.6404 test_acc: 0.2649

GNN Epoch [2200 / 5000] loss_train: 0.39563 accuracy_train: 0.85788
 test_loss: 2.6571 test_acc: 0.2798
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇███
wandb:         test_acc ▃█▇▄▆▅▂▆▄▅▁▇▄▅▆▄▆▅▅▅▅▇▇▅▅▄▄▅▄▅▄▂▇▇▆▆▆▅▃▆
wandb:       test_auroc ▁
wandb:          test_f1 ▁
wandb:        test_loss ▂▄▁▁▂▄▅▃▃▃▅▃▅▆▆▆▆▆▆▆▆▇▇▆▇▇▇▆▇▇████▅▆▆▆▄▆
wandb: test_macro_auroc ▁
wandb:    test_macro_f1 ▁
wandb:        test_prec ▁
wandb:         test_rec ▁
wandb:        train_acc ▁▁▄▂▇█▄▇▅▆▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇
wandb:       train_loss ▄█▂▁▁▃▂▂▂▃▂▂▂▂▂▂▂▁▁▁▇▂▁▁▁▁▁▁▁▄▃▂▂▂▂▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:            epoch 5000
wandb:         test_acc 0.3661
wandb:       test_auroc 0.5962
wandb:          test_f1 0.3316
wandb:        test_loss 2.60261
wandb: test_macro_auroc 0.5962
wandb:    test_macro_f1 0.3316
wandb:        test_prec 0.3319
wandb:         test_rec 0.3657
wandb:        train_acc 0.84638
wandb:       train_loss 0.41823
wandb: 
wandb: 🚀 View run adni_ct_gcn_SMOTE_full_fold_4 at: https://wandb.ai/ahnha_ahnha/adni_ct_gcn/runs/ql9q11gu
wandb: ⭐️ View project at: https://wandb.ai/ahnha_ahnha/adni_ct_gcn
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250715_031422-ql9q11gu/logs
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /home/user14/bagle/classifier/wandb/run-20250715_031724-5ajon5u0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run adni_ct_gcn_SMOTE_full_fold_5
wandb: ⭐️ View project at https://wandb.ai/ahnha_ahnha/adni_ct_gcn
wandb: 🚀 View run at https://wandb.ai/ahnha_ahnha/adni_ct_gcn/runs/5ajon5u0

GNN Epoch [2300 / 5000] loss_train: 0.38199 accuracy_train: 0.85455
 test_loss: 2.7733 test_acc: 0.2589

GNN Epoch [2400 / 5000] loss_train: 0.35814 accuracy_train: 0.87347
 test_loss: 2.8342 test_acc: 0.2738

GNN Epoch [2500 / 5000] loss_train: 1.92092 accuracy_train: 0.43117
 test_loss: 2.7454 test_acc: 0.3125

GNN Epoch [2600 / 5000] loss_train: 0.43184 accuracy_train: 0.83970
 test_loss: 2.4232 test_acc: 0.2976

GNN Epoch [2700 / 5000] loss_train: 0.38241 accuracy_train: 0.86902
 test_loss: 2.6111 test_acc: 0.3125

GNN Epoch [2800 / 5000] loss_train: 0.38555 accuracy_train: 0.85863
 test_loss: 2.5333 test_acc: 0.2917

GNN Epoch [2900 / 5000] loss_train: 0.34047 accuracy_train: 0.87904
 test_loss: 2.7316 test_acc: 0.3036

GNN Epoch [3000 / 5000] loss_train: 0.36424 accuracy_train: 0.86382
 test_loss: 2.7831 test_acc: 0.2827

GNN Epoch [3100 / 5000] loss_train: 0.46619 accuracy_train: 0.82894
 test_loss: 2.8526 test_acc: 0.2768

GNN Epoch [3200 / 5000] loss_train: 0.64029 accuracy_train: 0.76660
 test_loss: 3.1875 test_acc: 0.2768

GNN Epoch [3300 / 5000] loss_train: 0.35626 accuracy_train: 0.87532
 test_loss: 2.7254 test_acc: 0.2857

GNN Epoch [3400 / 5000] loss_train: 0.34217 accuracy_train: 0.87532
 test_loss: 2.8698 test_acc: 0.2708

GNN Epoch [3500 / 5000] loss_train: 0.33785 accuracy_train: 0.88423
 test_loss: 2.8951 test_acc: 0.2708

GNN Epoch [3600 / 5000] loss_train: 0.32588 accuracy_train: 0.88312
 test_loss: 2.9029 test_acc: 0.2768

GNN Epoch [3700 / 5000] loss_train: 0.34589 accuracy_train: 0.87644
 test_loss: 2.8615 test_acc: 0.2738

GNN Epoch [3800 / 5000] loss_train: 0.65855 accuracy_train: 0.75176
 test_loss: 1.8452 test_acc: 0.3125

GNN Epoch [3900 / 5000] loss_train: 0.49216 accuracy_train: 0.82375
 test_loss: 2.2684 test_acc: 0.3036

GNN Epoch [4000 / 5000] loss_train: 0.45949 accuracy_train: 0.83970
 test_loss: 2.4142 test_acc: 0.3006

GNN Epoch [4100 / 5000] loss_train: 0.52247 accuracy_train: 0.80668
 test_loss: 2.5948 test_acc: 0.2946

GNN Epoch [4200 / 5000] loss_train: 0.43479 accuracy_train: 0.84045
 test_loss: 2.4769 test_acc: 0.2946

GNN Epoch [4300 / 5000] loss_train: 0.43001 accuracy_train: 0.84453
 test_loss: 2.5528 test_acc: 0.2946

GNN Epoch [4400 / 5000] loss_train: 0.42622 accuracy_train: 0.84416
 test_loss: 2.4106 test_acc: 0.2917

GNN Epoch [4500 / 5000] loss_train: 0.46949 accuracy_train: 0.82041
 test_loss: 2.5862 test_acc: 0.3065

GNN Epoch [4600 / 5000] loss_train: 0.74246 accuracy_train: 0.71837
 test_loss: 2.6799 test_acc: 0.2619

GNN Epoch [4700 / 5000] loss_train: 0.40188 accuracy_train: 0.85417
 test_loss: 2.4734 test_acc: 0.2946

GNN Epoch [4800 / 5000] loss_train: 0.41038 accuracy_train: 0.85380
 test_loss: 2.5523 test_acc: 0.2976

GNN Epoch [4900 / 5000] loss_train: 0.46308 accuracy_train: 0.82931
 test_loss: 2.6764 test_acc: 0.3125

GNN Epoch [5000 / 5000] loss_train: 0.41823 accuracy_train: 0.84638
 test_loss: 2.6026 test_acc: 0.2857
GNN Training completed! Best accuracy: 0.3661
GNN load_and_test called with model_path: ./logs/20250715_030510/3.pth
Loading GNN model from: ./logs/20250715_030510/3.pth
GPU memory cleared after fold 4


=============================== Fold 5 ===============================
Loaded synthetic data from SMOTE_full_fold4.pt: 1383 samples
SMOTE synthetic data - using class-based adjacency matrices
Using SMOTE_full augmentation: 1383 additional samples
Applying GCN-specific adjacency processing to real data
Applying percentile-based sparsification and normalization for GCN (normalize=False)
Adding 1383 augmented samples to training set
Generating adjacency matrices for 1383 synthetic samples using average method
Creating class-wise average adjacency matrices with 90.0% sparsification
Class 0: processed 371 matrices, original avg degree: 1.05, target edges: 84
Class 1: processed 163 matrices, original avg degree: 1.05, target edges: 84
Class 2: processed 401 matrices, original avg degree: 1.05, target edges: 84
Class 3: processed 238 matrices, original avg degree: 1.05, target edges: 84
Class 4: processed 139 matrices, original avg degree: 1.05, target edges: 84
Assigning class average adjacency matrices for 1383 synthetic samples (Option-M)
Average adjacency assignment:
  Class 0: 168 synthetic samples
  Class 1: 376 synthetic samples
  Class 2: 138 synthetic samples
  Class 3: 301 synthetic samples
  Class 4: 400 synthetic samples
Final training set size: 2695 samples

GNN Epoch [100 / 5000] loss_train: 0.94168 accuracy_train: 0.62597
 test_loss: 1.6934 test_acc: 0.2440

GNN Epoch [200 / 5000] loss_train: 0.76732 accuracy_train: 0.70501
 test_loss: 1.8065 test_acc: 0.3253

GNN Epoch [300 / 5000] loss_train: 0.55968 accuracy_train: 0.79184
 test_loss: 1.8828 test_acc: 0.2952

GNN Epoch [400 / 5000] loss_train: 0.36819 accuracy_train: 0.86308
 test_loss: 2.0275 test_acc: 0.2922

GNN Epoch [500 / 5000] loss_train: 0.36878 accuracy_train: 0.87755
 test_loss: 2.0726 test_acc: 0.2922

GNN Epoch [600 / 5000] loss_train: 0.23817 accuracy_train: 0.91837
 test_loss: 2.4507 test_acc: 0.3163

GNN Epoch [700 / 5000] loss_train: 0.51102 accuracy_train: 0.81633
 test_loss: 1.9826 test_acc: 0.3223

GNN Epoch [800 / 5000] loss_train: 0.40142 accuracy_train: 0.86345
 test_loss: 2.5399 test_acc: 0.2952

GNN Epoch [900 / 5000] loss_train: 0.41467 accuracy_train: 0.85232
 test_loss: 2.6512 test_acc: 0.2922

GNN Epoch [1000 / 5000] loss_train: 0.30679 accuracy_train: 0.89536
 test_loss: 2.8382 test_acc: 0.3102

GNN Epoch [1100 / 5000] loss_train: 0.28711 accuracy_train: 0.89462
 test_loss: 2.5677 test_acc: 0.3193

GNN Epoch [1200 / 5000] loss_train: 0.23811 accuracy_train: 0.91540
 test_loss: 2.6401 test_acc: 0.3313

GNN Epoch [1300 / 5000] loss_train: 0.23649 accuracy_train: 0.91725
 test_loss: 2.7742 test_acc: 0.3042

GNN Epoch [1400 / 5000] loss_train: 0.22196 accuracy_train: 0.93173
 test_loss: 2.9820 test_acc: 0.2620

GNN Epoch [1500 / 5000] loss_train: 0.57063 accuracy_train: 0.79221
 test_loss: 2.0900 test_acc: 0.3163

GNN Epoch [1600 / 5000] loss_train: 0.45548 accuracy_train: 0.83636
 test_loss: 2.2098 test_acc: 0.3072

GNN Epoch [1700 / 5000] loss_train: 0.41009 accuracy_train: 0.85269
 test_loss: 2.3748 test_acc: 0.3373

GNN Epoch [1800 / 5000] loss_train: 0.51144 accuracy_train: 0.80186
 test_loss: 2.5017 test_acc: 0.2560

GNN Epoch [1900 / 5000] loss_train: 0.35941 accuracy_train: 0.86679
 test_loss: 2.5073 test_acc: 0.3102

GNN Epoch [2000 / 5000] loss_train: 0.32273 accuracy_train: 0.88868
 test_loss: 2.6250 test_acc: 0.3102

GNN Epoch [2100 / 5000] loss_train: 0.36592 accuracy_train: 0.85603
 test_loss: 2.7514 test_acc: 0.3133

GNN Epoch [2200 / 5000] loss_train: 0.45609 accuracy_train: 0.83562
 test_loss: 2.2327 test_acc: 0.3223

GNN Epoch [2300 / 5000] loss_train: 0.34065 accuracy_train: 0.87829
 test_loss: 2.5168 test_acc: 0.2831

GNN Epoch [2400 / 5000] loss_train: 0.35535 accuracy_train: 0.88126
 test_loss: 2.4706 test_acc: 0.2922

GNN Epoch [2500 / 5000] loss_train: 0.30237 accuracy_train: 0.89499
 test_loss: 2.7002 test_acc: 0.2801

GNN Epoch [2600 / 5000] loss_train: 0.30000 accuracy_train: 0.89128
 test_loss: 2.6978 test_acc: 0.3012

GNN Epoch [2700 / 5000] loss_train: 0.40127 accuracy_train: 0.85677
 test_loss: 2.7819 test_acc: 0.3012

GNN Epoch [2800 / 5000] loss_train: 0.25574 accuracy_train: 0.90872
 test_loss: 2.7285 test_acc: 0.3072

GNN Epoch [2900 / 5000] loss_train: 0.28197 accuracy_train: 0.89833
 test_loss: 2.6285 test_acc: 0.3343

GNN Epoch [3000 / 5000] loss_train: 0.29353 accuracy_train: 0.89239
 test_loss: 2.6463 test_acc: 0.3223

GNN Epoch [3100 / 5000] loss_train: 0.24238 accuracy_train: 0.91317
 test_loss: 2.8198 test_acc: 0.2952

GNN Epoch [3200 / 5000] loss_train: 0.24116 accuracy_train: 0.91725
 test_loss: 2.7802 test_acc: 0.3072

GNN Epoch [3300 / 5000] loss_train: 0.48644 accuracy_train: 0.82968
 test_loss: 2.3319 test_acc: 0.3042

GNN Epoch [3400 / 5000] loss_train: 0.40419 accuracy_train: 0.85529
 test_loss: 2.4070 test_acc: 0.2952

GNN Epoch [3500 / 5000] loss_train: 0.26496 accuracy_train: 0.90872
 test_loss: 2.6599 test_acc: 0.2892

GNN Epoch [3600 / 5000] loss_train: 0.31878 accuracy_train: 0.88794
wandb: uploading config.yaml
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            epoch ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇██
wandb:         test_acc ▄▄▅▃▄▄█▆▅▆▄▄▄▅▅▂▄▃▅▂▅▄▅▅▄▃▅▄▄▄▆▅▃▃▃▂▂▃▆▁
wandb:       test_auroc ▁
wandb:          test_f1 ▁
wandb:        test_loss ▁▂▁▃▂▅▅▆▅▆▅▆▅▇▆▆▇▆▇▇▇▆██▆▇▇█▇▇▁▂▂▃▄▂▃▃▄▅
wandb: test_macro_auroc ▁
wandb:    test_macro_f1 ▁
wandb:        test_prec ▁
wandb:         test_rec ▁
wandb:        train_acc ▁▄▆▆▇▃▇▇▇███▃▂▆▆▇▇▆▇█▆▆▆▇▇▇███▇█▇▇▇▃▆▇▅▇
wandb:       train_loss ▇█▃▇▃▂▂▂▂▁▃▂▂▂▃▂▂▂▁▅▂▂▁▁▁▁▇▆▁▁█▆▄▄▄▃▅▅▄▄
wandb: 
wandb: Run summary:
wandb:            epoch 5000
wandb:         test_acc 0.3735
wandb:       test_auroc 0.5979
wandb:          test_f1 0.3061
wandb:        test_loss 2.07847
wandb: test_macro_auroc 0.5979
wandb:    test_macro_f1 0.3061
wandb:        test_prec 0.3253
wandb:         test_rec 0.3456
wandb:        train_acc 0.83933
wandb:       train_loss 0.43628
wandb: 
wandb: 🚀 View run adni_ct_gcn_SMOTE_full_fold_5 at: https://wandb.ai/ahnha_ahnha/adni_ct_gcn/runs/5ajon5u0
wandb: ⭐️ View project at: https://wandb.ai/ahnha_ahnha/adni_ct_gcn
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250715_031724-5ajon5u0/logs
 test_loss: 2.3879 test_acc: 0.3193

GNN Epoch [3700 / 5000] loss_train: 0.24856 accuracy_train: 0.92022
 test_loss: 2.7444 test_acc: 0.3072

GNN Epoch [3800 / 5000] loss_train: 0.23770 accuracy_train: 0.91391
 test_loss: 3.0668 test_acc: 0.3102

GNN Epoch [3900 / 5000] loss_train: 0.24533 accuracy_train: 0.91651
 test_loss: 2.7355 test_acc: 0.2801

GNN Epoch [4000 / 5000] loss_train: 0.23452 accuracy_train: 0.91688
 test_loss: 3.0032 test_acc: 0.2831

GNN Epoch [4100 / 5000] loss_train: 0.94918 accuracy_train: 0.62078
 test_loss: 1.6569 test_acc: 0.3012

GNN Epoch [4200 / 5000] loss_train: 0.56812 accuracy_train: 0.79443
 test_loss: 1.8664 test_acc: 0.2831

GNN Epoch [4300 / 5000] loss_train: 0.46651 accuracy_train: 0.82746
 test_loss: 2.0003 test_acc: 0.2922

GNN Epoch [4400 / 5000] loss_train: 0.41852 accuracy_train: 0.85269
 test_loss: 2.1449 test_acc: 0.2861

GNN Epoch [4500 / 5000] loss_train: 1.18887 accuracy_train: 0.48683
 test_loss: 2.0254 test_acc: 0.1958

GNN Epoch [4600 / 5000] loss_train: 0.58327 accuracy_train: 0.76846
 test_loss: 1.9806 test_acc: 0.2892

GNN Epoch [4700 / 5000] loss_train: 0.42741 accuracy_train: 0.84341
 test_loss: 2.1792 test_acc: 0.2801

GNN Epoch [4800 / 5000] loss_train: 0.41044 accuracy_train: 0.86011
 test_loss: 2.2530 test_acc: 0.2861

GNN Epoch [4900 / 5000] loss_train: 0.59001 accuracy_train: 0.76846
 test_loss: 1.8639 test_acc: 0.2741

GNN Epoch [5000 / 5000] loss_train: 0.43628 accuracy_train: 0.83933
 test_loss: 2.0785 test_acc: 0.2922
GNN Training completed! Best accuracy: 0.3735
GNN load_and_test called with model_path: ./logs/20250715_030510/4.pth
Loading GNN model from: ./logs/20250715_030510/4.pth
GPU memory cleared after fold 5
--------------- Result ---------------
Label distribution:   Counter({2: 505, 0: 467, 3: 309, 1: 194, 4: 169})
5-Fold test loss:     [2.955293655395508, 1.904799461364746, 1.5826654434204102, 1.926015019416809, 2.5814855098724365]
5-Fold test accuracy: [0.382089552238806, 0.375, 0.3706070287539936, 0.36607142857142855, 0.3734939759036145]
---------- Confusion Matrix ----------
5-Fold precision:     [0.2773612023243667, 0.28904924043196495, 0.28012363943701724, 0.33190571128673196, 0.3253299199664489]
5-Fold specificity:   [0.8305166517313181, 0.8327846958210972, 0.8288835178662047, 0.8307454471504816, 0.8352210370605382]
5-Fold sensitivity:   [0.2963434382236171, 0.2483102614809932, 0.32220455749867516, 0.36574929160579756, 0.3456082564778217]
5-Fold f1 score:      [0.2699425878908484, 0.4136308136308136, 0.33514214129474085, 0.3315882738855053, 0.30609797352597756]
5-Fold AUROC:         [0.577938706675804, 0.5959829949896116, 0.6203455778605995, 0.5962270308718863, 0.5978529318625619]
5-Fold Macro F1:      [0.2699425878908484, 0.24817848817848817, 0.2681137130357927, 0.3315882738855053, 0.30609797352597745]
5-Fold Macro AUROC:   [0.577938706675804, 0.5959829949896116, 0.6203455778605995, 0.5962270308718863, 0.5978529318625619]
-------------- Mean, Std --------------
Acc:   0.3735 ± 0.0053
Prec:  0.3008 ± 0.0232
Rec:   0.3156 ± 0.0409
F1:    0.3313 ± 0.0473
AUROC: 0.5977 ± 0.0135
Macro F1: 0.2848 ± 0.0299
Macro AUROC: 0.5977 ± 0.0135
Results saved to /home/user14/bagle/summary/experiment_summary.xlsx
Excel summary saved to: /home/user14/bagle/summary/experiment_summary.xlsx
GPU memory cleared after experiment completion

Time: 0:15:14
Final GPU memory cleanup completed

종료 시간: Tue Jul 15 03:20:26 UTC 2025
종료 코드: 0
✓ 실험 완료: gcn_SMOTE_full_average
